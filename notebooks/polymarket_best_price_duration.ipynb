{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Polymarket best-price duration analysis\n\nThis notebook studies **how long the best prices (best bid / best ask) remain unchanged** in Polymarket order books. We focus on per-market behavior with a deep dive into a selected market, plus optional aggregates across multiple markets for broader context.\n\n**Goal:** quantify the *persistence* of top-of-book prices by measuring the duration of each run where the best bid or best ask stays constant.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nimport sys\nimport subprocess\nimport importlib.util\nfrom pathlib import Path\n\n# Install lightweight dependencies once per environment\n\ndef ensure_package(pkg_name: str) -> None:\n    if importlib.util.find_spec(pkg_name) is None:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n\nfor pkg in (\"pandas\", \"sqlalchemy\", \"python-dotenv\", \"psycopg2-binary\", \"plotly\", \"numpy\"):\n    ensure_package(pkg)\n\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom dotenv import load_dotenv\nimport plotly.express as px\nimport plotly.graph_objects as go\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load DATABASE_URL from repo .env if present\nroot = Path.cwd()\nload_dotenv(root / \".env\")\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nif DATABASE_URL and DATABASE_URL.startswith(\"postgres://\"):\n    DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)\nif not DATABASE_URL:\n    raise RuntimeError(\"DATABASE_URL not set. Add to .env or environment.\")\n\nengine = create_engine(DATABASE_URL)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Market overview\n\nWe start by finding markets with the most orderbook events. Pick a `market_instance_id` to analyze in depth.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "market_overview_query = \"\"\"\nSELECT\n    market_instance_id,\n    market,\n    asset_label,\n    COUNT(*) AS num_events,\n    MIN(event_timestamp_ms) AS min_ts,\n    MAX(event_timestamp_ms) AS max_ts\nFROM polymarket_orderbook_events\nGROUP BY market_instance_id, market, asset_label\nORDER BY num_events DESC\nLIMIT 25;\n\"\"\"\n\nmarket_overview = pd.read_sql(market_overview_query, engine)\nmarket_overview[\"span_minutes\"] = (market_overview[\"max_ts\"] - market_overview[\"min_ts\"]) / 60000.0\nmarket_overview\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Select a market to analyze\n\nChoose a `market_instance_id` from the table above. This analysis is designed to be deep on one market to understand best-price persistence in detail.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Pick the market with the most events by default\nselected_market = market_overview.iloc[0][\"market_instance_id\"]\nselected_market\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Load orderbook events for the selected market\n\nWe pull best bid/ask prices per event for each side (UP/DOWN).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "events_query = \"\"\"\nSELECT\n    event_timestamp_ms,\n    side,\n    best_bid_price,\n    best_ask_price,\n    best_bid_qty,\n    best_ask_qty\nFROM polymarket_orderbook_events\nWHERE market_instance_id = %(market)s\nORDER BY event_timestamp_ms ASC;\n\"\"\"\n\npm = pd.read_sql(events_query, engine, params={\"market\": selected_market})\nif pm.empty:\n    raise RuntimeError(\"No orderbook data found for selected market.\")\n\npm[\"event_ts\"] = pd.to_datetime(pm[\"event_timestamp_ms\"], unit=\"ms\", utc=True)\npm.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Helper: compute best-price duration runs\n\nWe measure how long each *unchanged* price run lasts. For each side, we compute consecutive runs where the best bid or best ask stays the same, and measure the duration until the next price change.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def price_run_durations(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n    # df must be sorted by event_timestamp_ms\n    df = df.copy()\n    df = df.sort_values(\"event_timestamp_ms\")\n\n    # Identify changes in price\n    price_change = df[price_col].ne(df[price_col].shift(1))\n    df[\"run_id\"] = price_change.cumsum()\n\n    # For each run, take the first timestamp and price\n    run_summary = (\n        df.groupby(\"run_id\")[[\"event_timestamp_ms\", price_col]]\n        .first()\n        .rename(columns={\"event_timestamp_ms\": \"run_start_ms\", price_col: \"price\"})\n        .reset_index(drop=True)\n    )\n\n    # Run end is the start of the next run\n    run_summary[\"run_end_ms\"] = run_summary[\"run_start_ms\"].shift(-1)\n    run_summary[\"duration_ms\"] = run_summary[\"run_end_ms\"] - run_summary[\"run_start_ms\"]\n\n    # Drop final run (no end timestamp)\n    run_summary = run_summary.dropna(subset=[\"duration_ms\"])\n    run_summary[\"duration_sec\"] = run_summary[\"duration_ms\"] / 1000.0\n    return run_summary\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Best bid duration analysis\n\nThis tells us how long the **best bid price** stays unchanged before moving.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "bid_runs = []\nfor side, group in pm.groupby(\"side\"):\n    runs = price_run_durations(group, \"best_bid_price\")\n    runs[\"side\"] = side\n    runs[\"price_type\"] = \"best_bid\"\n    bid_runs.append(runs)\n\nbid_runs = pd.concat(bid_runs, ignore_index=True)\nbid_runs.head()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "bid_summary = (\n    bid_runs.groupby(\"side\")[\"duration_sec\"]\n    .describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n    .reset_index()\n)\nbid_summary\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig = px.histogram(\n    bid_runs,\n    x=\"duration_sec\",\n    color=\"side\",\n    nbins=60,\n    title=\"Best Bid Price Persistence (seconds)\",\n)\nfig.update_layout(xaxis_title=\"Duration (sec)\", yaxis_title=\"Count\")\nfig.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Best ask duration analysis\n\nThis tells us how long the **best ask price** stays unchanged before moving.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "ask_runs = []\nfor side, group in pm.groupby(\"side\"):\n    runs = price_run_durations(group, \"best_ask_price\")\n    runs[\"side\"] = side\n    runs[\"price_type\"] = \"best_ask\"\n    ask_runs.append(runs)\n\nask_runs = pd.concat(ask_runs, ignore_index=True)\nask_runs.head()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "ask_summary = (\n    ask_runs.groupby(\"side\")[\"duration_sec\"]\n    .describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n    .reset_index()\n)\nask_summary\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig = px.histogram(\n    ask_runs,\n    x=\"duration_sec\",\n    color=\"side\",\n    nbins=60,\n    title=\"Best Ask Price Persistence (seconds)\",\n)\nfig.update_layout(xaxis_title=\"Duration (sec)\", yaxis_title=\"Count\")\nfig.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Compare bid vs ask persistence\n\nWe can compare distributions to see whether bids or asks tend to be more stable.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "combined_runs = pd.concat([bid_runs, ask_runs], ignore_index=True)\n\nfig = px.box(\n    combined_runs,\n    x=\"price_type\",\n    y=\"duration_sec\",\n    color=\"side\",\n    points=\"outliers\",\n    title=\"Best Price Duration by Side and Type\",\n)\nfig.update_layout(yaxis_title=\"Duration (sec)\")\nfig.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Share of time at a given best price\n\nWe can estimate how concentrated time is in the most persistent price levels by weighting durations. This reveals whether a few price levels dominate the time spent at top-of-book.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def top_price_concentration(runs: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:\n    grouped = (\n        runs.groupby([\"side\", \"price\"])[\"duration_sec\"]\n        .sum()\n        .reset_index()\n        .sort_values([\"side\", \"duration_sec\"], ascending=[True, False])\n    )\n    totals = grouped.groupby(\"side\")[\"duration_sec\"].sum().rename(\"total_sec\")\n    grouped = grouped.merge(totals, on=\"side\")\n    grouped[\"share\"] = grouped[\"duration_sec\"] / grouped[\"total_sec\"]\n    return grouped.groupby(\"side\").head(top_n)\n\nconcentration_bid = top_price_concentration(bid_runs, top_n=5)\nconcentration_ask = top_price_concentration(ask_runs, top_n=5)\n\nconcentration_bid\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "concentration_ask\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Optional: Multi-market aggregate (SQL window approach)\n\nFor a broader view, this SQL computes *run durations* in the database using window functions. This can be heavy depending on dataset size, so start with a small limit or filter.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "aggregate_query = \"\"\"\nWITH ordered AS (\n    SELECT\n        market_instance_id,\n        side,\n        event_timestamp_ms,\n        best_bid_price,\n        best_ask_price,\n        LAG(best_bid_price) OVER (PARTITION BY market_instance_id, side ORDER BY event_timestamp_ms) AS prev_bid,\n        LAG(best_ask_price) OVER (PARTITION BY market_instance_id, side ORDER BY event_timestamp_ms) AS prev_ask\n    FROM polymarket_orderbook_events\n), runs AS (\n    SELECT\n        *,\n        CASE WHEN best_bid_price = prev_bid THEN 0 ELSE 1 END AS bid_change,\n        CASE WHEN best_ask_price = prev_ask THEN 0 ELSE 1 END AS ask_change\n    FROM ordered\n), bid_runs AS (\n    SELECT\n        market_instance_id,\n        side,\n        event_timestamp_ms,\n        best_bid_price,\n        SUM(bid_change) OVER (PARTITION BY market_instance_id, side ORDER BY event_timestamp_ms) AS bid_run_id\n    FROM runs\n), bid_run_bounds AS (\n    SELECT\n        market_instance_id,\n        side,\n        bid_run_id,\n        MIN(event_timestamp_ms) AS run_start_ms\n    FROM bid_runs\n    GROUP BY market_instance_id, side, bid_run_id\n), bid_run_durations AS (\n    SELECT\n        market_instance_id,\n        side,\n        run_start_ms,\n        LEAD(run_start_ms) OVER (PARTITION BY market_instance_id, side ORDER BY run_start_ms) AS run_end_ms\n    FROM bid_run_bounds\n)\nSELECT\n    side,\n    COUNT(*) AS num_runs,\n    AVG((run_end_ms - run_start_ms) / 1000.0) AS avg_duration_sec,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY (run_end_ms - run_start_ms) / 1000.0) AS median_duration_sec\nFROM bid_run_durations\nWHERE run_end_ms IS NOT NULL\nGROUP BY side;\n\"\"\"\n\n# Uncomment to run if your database can handle a larger aggregate\n# aggregate_summary = pd.read_sql(aggregate_query, engine)\n# aggregate_summary\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Interpretation prompts\n\nUse these prompts to guide your analysis and writeup:\n\n- Are best bids more stable than best asks? Does this differ by side (UP vs DOWN)?\n- Do a few price levels account for most of the time at top-of-book?\n- Are there notable asymmetries between UP and DOWN sides?\n- Does price persistence change during high-volatility windows (you can filter by time to test)?\n\nFeel free to extend the notebook with additional filters (time windows, volume thresholds) or to compare multiple markets using the aggregate SQL approach above.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}